[connection-aws-credentials]: https://github.com/jazracherif/udacity-data-engineer-airflow/blob/master/docs/connection-aws-credentials.png
[connection-redshift]: https://github.com/jazracherif/udacity-data-engineer-airflow/blob/master/docs/connection-redshift.png
[dag]: https://github.com/jazracherif/udacity-data-engineer-airflow/blob/master/docs/dag.png
[postico]: https://github.com/jazracherif/udacity-data-engineer-airflow/blob/master/docs/postico.png

# Udacity Data Engineer Nanodegree - Airflow Project

In this project, I create an ETL Pipeline using Airflow, that ingests and transforms data from S3 into AWS Redshift tables.

Two datasets are used:
1. A song dataset, which is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/).
2. A log dataset, which consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above.


Both files are stored in S3 bucket `s3://udacity-dend` and are ingested into fact and dimension tables.

The project consists of the following files:
- redshift.py: helps manage creating and tear down of a redshift cluster.
- create_tables.py: helps create the Redshift fact and dimension tables.
- etl.py: helps run the ETL pipeline extracting facts and dimension data from the S3 files.

The final product of the pipeline consist of the following tables:

Fact Table

    songplays - records in event data associated with song plays i.e. records with page NextSong
        songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

Dimension Tables

    users - users in the app
        user_id, first_name, last_name, gender, level
    songs - songs in music database
        song_id, title, artist_id, year, duration
    artists - artists in music database
        artist_id, name, location, lattitude, longitude
    time - timestamps of records in songplays broken down into specific units
        start_time, hour, day, week, month, year, weekday

## Installation

Create Virtual Environment and Install the packages

`make install`

Run Airflow:

`make start`

Create a Redshift cluster:

`python redshift --cmd create`

## Setup Cryptographic Key

As per the airflow [instruction](https://airflow.readthedocs.io/en/stable/howto/secure-connections.html)

generate fernet instruction by running the following in python console:

    from cryptography.fernet import Fernet
    fernet_key= Fernet.generate_key()
    print(fernet_key.decode()) # your fernet_key, keep it in secured place!

and copy the output into `fernet_key` in file `./fernet.key`. Make sure not to commit this back to github

## Table Initialization

Download [Postico](https://eggerapps.at/postico/) or similar Postgres Client and create a connection with the Redshift hostname, username, password, and port.

![postico][postico]

Run the `create_tables.sql` in Postico once before enabling the DAG. This will delete and recreate the tables used by the pipeline.

## Add Airflow Connections

Go to localhost:8080 in your browser and click on the Admin tab and select Connections.

Under Connections, select Create.

On the create connection page, enter the following values:
* Conn Id: Enter aws_credentials.
* Conn Type: Enter Amazon Web Services.
* Login: Enter your Access key ID from the IAM User credentials you downloaded earlier.
* Password: Enter your Secret access key from the IAM User credentials you downloaded earlier.

![connection-aws-credentials][connection-aws-credentials]

Once you've entered these values, select Save and Add Another.

On the next create connection page, enter the following values:
* Conn Id: Enter redshift.
* Conn Type: Enter Postgres.
* Host: Enter the endpoint of your Redshift cluster, excluding the port at the end.
* Schema: Enter dev. This is the Redshift database you want to connect to.
* Login: Enter awsuser.
* Password: Enter the password you created when launching your Redshift cluster.
* Port: Enter 5439.

Once you've entered these values, select Save.

![connection redshift][connection-redshift]


## Running the Pipeline

The DAG should appear under the name `sparkify-etl-v1`. Once enabled, it will run daily for the period 11/01/2018 to 11/30/2018. To change the schedule, go to airflow/dags/sparkify-etl.py and look for the following arguments:
- start_date
- end_date

![data][dag]

## Analysis

Using Postico, We can run the following query at the end of the Pipeline for analysis.


we can find the top 5 power users with the following query:

~~~ sql
WITH top_users AS (
    SELECT user_id, COUNT(*) AS count
    FROM songplay
    GROUP BY user_id
    ORDER BY cnt DESC
    LIMIT 5
)
SELECT users.first_name, 
       users.last_name, 
       top_users.cnt
  FROM top_users
 INNER JOIN users
       ON users.user_id = top_users.user_id
 ORDER BY cnt DESC
~~~~

and we get:

| first_name | last_name | cnt |
| ------------- |:-------------:|:-------------:|
| Chloe | Cuevas | 41 |
| Tegan | Levine | 31 |
| Kate | Harrell | 28 |
| Lily | Koch | 20 | 
| Aleena | Kirby | 18 |

We can also look for the top 5 most popular locations where songs are played, using the following query:

~~~ sql
SELECT location, 
       count(*) AS cnt 
  FROM songplays
 GROUP BY location 
 ORDER BY cnt DESC 
 LIMIT 5
~~~~

| location | count |
| ------------- |:-------------:|
| San Francisco-Oakland-Hayward, CA | 41
| Portland-South Portland, ME | 31
| Lansing-East Lansing, MI | 28
| Chicago-Naperville-Elgin, IL-IN-WI | 20
| Atlanta-Sandy Springs-Roswell, GA | 18


