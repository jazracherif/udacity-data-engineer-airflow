[connection-aws-credentials]: connection-aws-credentials.png
[connection-redshift]: connection-redshift.png
[dag]: dag.png
[postico]: postico.png

# Udacity Data Engineer Nanodegree - Airflow Project

In this project, I create an ETL Pipeline using Airflow, that ingests and transforms data from S3 into AWS Redshift tables.

Two datasets are used:
1. A song dataset, which is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/).
2. A log dataset, which consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above.


Both files are stored in S3 bucket `s3://udacity-dend` and are ingested into fact and dimension tables.

The project consists of the following files:
- redshift.py: helps manage creating and tear down of a redshift cluster.
- create_tables.py: helps create the Redshift fact and dimension tables.
- etl.py: helps run the ETL pipeline extracting facts and dimension data from the S3 files.

The final product of the pipeline consist of the following tables:

Fact Table

    songplays - records in event data associated with song plays i.e. records with page NextSong
        songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

Dimension Tables

    users - users in the app
        user_id, first_name, last_name, gender, level
    songs - songs in music database
        song_id, title, artist_id, year, duration
    artists - artists in music database
        artist_id, name, location, lattitude, longitude
    time - timestamps of records in songplays broken down into specific units
        start_time, hour, day, week, month, year, weekday

## Installation

Create Virtual Environment and Install the packages

`make install`

Run Airflow:

`make start`

Create a Redshift cluster:

`python redshift --cmd create`

## Setup Cryptographic Key

As per the airflow [instruction](https://airflow.readthedocs.io/en/stable/howto/secure-connections.html)

generate fernet instruction by running the following in python console:

    from cryptography.fernet import Fernet
    fernet_key= Fernet.generate_key()
    print(fernet_key.decode()) # your fernet_key, keep it in secured place!

and copy the output into `fernet_key` in airflow.cfg

## Table Initialization

Download [Postico](https://eggerapps.at/postico/) or similar Postgres Client and create a connection with the Redshift hostname, username, password, and port.

![postico][postico]

Run the `create_tables.sql` in Postico once before enabling the DAG. This will delete and recreate the tables used by the pipeline.

## Add Airflow Connections

Go to localhost:8080 in your browser and click on the Admin tab and select Connections.

Under Connections, select Create.

On the create connection page, enter the following values:
* Conn Id: Enter aws_credentials.
* Conn Type: Enter Amazon Web Services.
* Login: Enter your Access key ID from the IAM User credentials you downloaded earlier.
* Password: Enter your Secret access key from the IAM User credentials you downloaded earlier.

![connection-aws-credentials][connection-aws-credentials]

Once you've entered these values, select Save and Add Another.

On the next create connection page, enter the following values:
* Conn Id: Enter redshift.
* Conn Type: Enter Postgres.
* Host: Enter the endpoint of your Redshift cluster, excluding the port at the end.
* Schema: Enter dev. This is the Redshift database you want to connect to.
* Login: Enter awsuser.
* Password: Enter the password you created when launching your Redshift cluster.
* Port: Enter 5439.

Once you've entered these values, select Save.

![connection redshift][connection-redshift]


## Running the Pipeline

The DAG should appear under the name `sparkify-etl-v1` and will run daily for the period 11/01/2018 to 11/30/2018:

![data][dag]

## Analysis


